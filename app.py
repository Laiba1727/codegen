from fastapi import FastAPI, HTTPException, Request, Response
from pydantic import BaseModel
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import autopep8
import subprocess
import time
import re
import os
from pathlib import Path
from fastapi.middleware.cors import CORSMiddleware
import logging
from typing import Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI with minimal settings
app = FastAPI(
    title="Code Evaluation API",
    docs_url=None,  # Disable docs to simplify
    redoc_url=None,
    openapi_url=None
)

# Enhanced CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["POST", "OPTIONS", "GET"],  # Explicitly allowed methods
    allow_headers=["*"],
    expose_headers=["*"]
)

# Environment Setup
CACHE_DIR = Path("./.cache/huggingface")
CACHE_DIR.mkdir(parents=True, exist_ok=True)
os.environ["TRANSFORMERS_CACHE"] = str(CACHE_DIR)
os.environ["HF_HOME"] = str(CACHE_DIR)

# Model Configuration
MODEL_NAME = "codellama/CodeLlama-1.0-7b-hf"
tokenizer: Optional[AutoTokenizer] = None
model: Optional[AutoModelForCausalLM] = None

# Request Model
class CodeRequest(BaseModel):
    code: str
    language: str = "python"

# Helper Functions
def evaluate_code(user_code: str, lang: str) -> dict:
    """Evaluate code for correctness, performance, and security"""
    try:
        start_time = time.time()
        file_ext = {"python": "py", "java": "java", "cpp": "cpp", "javascript": "js"}.get(lang, "txt")
        filename = f"temp_script.{file_ext}"

        with open(filename, "w") as f:
            f.write(user_code)

        commands = {
            "python": ["python3", filename],
            "java": ["javac", filename, "&&", "java", filename.replace(".java", "")],
            "cpp": ["g++", filename, "-o", "temp_script.out", "&&", "./temp_script.out"],
            "javascript": ["node", filename]
        }

        if lang in commands:
            result = subprocess.run(" ".join(commands[lang]), 
                                capture_output=True, 
                                text=True, 
                                timeout=5, 
                                shell=True)
            exec_time = time.time() - start_time
            correctness = 1 if result.returncode == 0 else 0
            error_message = None if correctness else result.stderr.strip()
        else:
            return {"status": "error", "message": "Unsupported language", "score": 0}

        # Scoring logic
        readability_score = 20 if len(user_code) < 200 else 10
        efficiency_score = 30 if exec_time < 1 else 10
        security_score = 20 if "eval(" not in user_code and "exec(" not in user_code else 0
        total_score = (correctness * 50) + readability_score + efficiency_score + security_score

        feedback = []
        if correctness == 0:
            feedback.append("âŒ Error in Code Execution! Check syntax or logic errors.")
            feedback.append(f"ðŸ“Œ Error Details: {error_message}")
        else:
            feedback.append("âœ… Code executed successfully!")

        if efficiency_score < 30:
            feedback.append("âš¡ Performance Issue: Code took longer to execute. Optimize loops or calculations.")
        if readability_score < 20:
            feedback.append("ðŸ“– Readability Issue: Code is lengthy. Break into smaller functions.")
        if security_score == 0:
            feedback.append("ðŸ”’ Security Risk: Avoid using eval() or exec().")

        return {
            "status": "success" if correctness else "error",
            "execution_time": round(exec_time, 3) if correctness else None,
            "score": max(0, min(100, total_score)),
            "feedback": "\n".join(feedback),
            "error_details": error_message if not correctness else None
        }
    except Exception as e:
        logger.error(f"Error in evaluate_code: {str(e)}")
        return {"status": "error", "message": str(e), "score": 0}

def optimize_code_ai(user_code: str, lang: str) -> str:
    """Generate optimized code using AI"""
    global tokenizer, model
    if tokenizer is None or model is None:
        logger.info("Loading model...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                MODEL_NAME,
                cache_dir=str(CACHE_DIR))
            
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                device_map="auto",
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                load_in_8bit=True,
                cache_dir=str(CACHE_DIR))
            logger.info("Model loaded successfully")
        except Exception as e:
            logger.error(f"Model loading failed: {str(e)}")
            raise RuntimeError(f"Failed to load model: {str(e)}")

    try:
        if lang == "python":
            user_code = autopep8.fix_code(user_code)
            user_code = re.sub(r"eval\((.*)\)", r"int(\1)  # Removed eval for security", user_code)
            user_code = re.sub(r"/ 0", "/ 1  # Fixed division by zero", user_code)
        
        prompt = f"Optimize this {lang} code:\n```{lang}\n{user_code}\n```\nOptimized version:"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=1024)
        
        optimized_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        code_match = re.search(r'```(?:python)?\n(.*?)\n```', optimized_code, re.DOTALL)
        if code_match:
            optimized_code = code_match.group(1)
        
        return optimized_code if optimized_code else user_code
    except Exception as e:
        logger.error(f"Error in optimize_code_ai: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI optimization failed: {str(e)}")

# API Endpoints - Direct routes without router
@app.post("/evaluate")
async def evaluate_endpoint(request: CodeRequest):
    try:
        result = evaluate_code(request.code, request.language)
        return {"status": "success", "result": result}
    except Exception as e:
        logger.error(f"Error in evaluate_endpoint: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/optimize")
async def optimize_endpoint(request: CodeRequest):
    try:
        optimized = optimize_code_ai(request.code, request.language)
        return {"status": "success", "optimized_code": optimized}
    except Exception as e:
        logger.error(f"Error in optimize_endpoint: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

# Explicit OPTIONS handlers
@app.options("/evaluate")
async def options_evaluate():
    return Response(
        status_code=200,
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            "Access-Control-Allow-Headers": "*"
        }
    )

@app.options("/optimize")
async def options_optimize():
    return Response(
        status_code=200,
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            "Access-Control-Allow-Headers": "*"
        }
    )

# Health check endpoint
@app.get("/health")
async def health_check():
    return Response(
        content="OK",
        media_type="text/plain",
        status_code=200
    )

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": "Code Evaluation API is running",
        "endpoints": {
            "evaluate": "POST /evaluate",
            "optimize": "POST /optimize"
        }
    }

# Request logging middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    logger.info(f"Request: {request.method} {request.url.path}")
    response = await call_next(request)
    logger.info(f"Response: {response.status_code}")
    return response

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8080))
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=port,
        workers=1,
        timeout_keep_alive=60
    )
